{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f6e4f49-cf3d-40fa-aa21-1b7c6fdeba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm  # For a nice progress bar!\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd8cd37c-36bd-460b-886e-75b00ed427dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset =  '/Users/tushar/iisc/sem3/TSE/project/Dataset/title_body.csv'\n",
    "df = pd.read_csv(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "332cc5cd-5b24-4318-bd26-5ffdda1d5eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the sentence\n",
    "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "def tokenize(df,max_length):\n",
    "    input_ids = []\n",
    "    for sent in tqdm(df):\n",
    "        encoded = tz.encode_plus(\n",
    "            text=sent,  # the sentence to be encoded\n",
    "            add_special_tokens=True,  # Add [CLS] and [SEP]\n",
    "            max_length = max_length,  # maximum length of a sentence\n",
    "            pad_to_max_length=True,  # Add [PAD]s\n",
    "            return_attention_mask = True,  # Generate the attention mask\n",
    "            truncation=True\n",
    "        )\n",
    "        # Get the input IDs and attention mask in tensor format\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        \n",
    "        \n",
    "    return torch.tensor(input_ids)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f0e8e-9e18-4ee9-a070-ba9de8d0c5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                             | 0/666576 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2198: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      " 89%|███████████████████████████████████████████████████████████████████████████████████████████████████            | 595133/666576 [31:49<04:50, 245.95it/s]"
     ]
    }
   ],
   "source": [
    "title_max_length = 150\n",
    "body_max_length = 1500 \n",
    "tags_max_length = 150 \n",
    "title_body_max_length = 1700\n",
    "df.replace(np.nan, '', inplace=True)\n",
    "# title_tensor = tokenize(df['Title'],title_max_length)\n",
    "# body_tensor = tokenize(df['Body'],body_max_length)\n",
    "# tags_tensor = tokenize(df['Tags'],tags_max_length)\n",
    "title_body_tensor = tokenize(df['title_body'],title_body_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04dc796e-0ffd-4a58-8935-1d1938c8edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(title_tensor, 'title_tensor.pt')\n",
    "torch.save(body_tensor, 'body_tensor.pt')\n",
    "torch.save(tags_tensor, 'tags_tensor.pt')\n",
    "torch.save(tz.vocab,'vocab.v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6782e245-959c-4967-8564-0b50268e59e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, txt, labels):\n",
    "        self.labels = labels\n",
    "        self.text = txt\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        text = self.text[idx]\n",
    "#         sample = {\"Text\": text, \"Class\": label}\n",
    "        return (text,label)\n",
    "\n",
    "# define data set object\n",
    "dataset = CustomTextDataset(title_tensor,df['closed'].to_numpy())\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab8e2bfa-9c01-4ed3-aa45-4741e449886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 256\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "sequence_length = title_tensor.size(1)\n",
    "learning_rate = 0.005\n",
    "batch_size = 2048\n",
    "num_epochs = 3\n",
    "\n",
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes,num_embeddings,embedding_size = 256):\n",
    "        super(RNN, self).__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings,embedding_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        \n",
    "        x = self.emb(x)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "\n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "176babc9-b74d-434f-9c1a-a9c6915c7e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 261/261 [15:15<00:00,  3.51s/it]\n",
      "100%|█████████████████████████████████████████| 261/261 [15:20<00:00,  3.53s/it]\n",
      "100%|█████████████████████████████████████████| 261/261 [15:18<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 54.631321\n",
      "Accuracy on test set: 54.38\n"
     ]
    }
   ],
   "source": [
    "# Initialize network (try out just using simple RNN, or GRU, and then compare with LSTM)\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes,len(tz.vocab)).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device).squeeze(1)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent update step/adam step\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    # Set model to eval\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device).squeeze(1)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    # Toggle model back to train\n",
    "    model.train()\n",
    "    return num_correct / num_samples\n",
    "\n",
    "\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a460a2-dfdb-48d1-930d-609ff796c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"RNN_title_0.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c58b6b1-bba0-49d8-b6e6-63e200207691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
